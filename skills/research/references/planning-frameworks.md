# Planning Frameworks

Reference for research planning: MECE decomposition, hypothesis-driven investigation, and research planning patterns.

---

## MECE Framework

**MECE** = Mutually Exclusive, Collectively Exhaustive

### Core Principles

- **Mutually Exclusive:** Categories don't overlap (no double-counting)
- **Collectively Exhaustive:** Categories cover all possibilities (no gaps)

### Why MECE Matters

Without MECE:
```
Research Question: "Why are users churning?"
Sub-questions (BAD):
- Are users unhappy with pricing? â† overlaps with value
- Is the product too hard to use? â† overlaps with UX
- Do users not see value? â† overlaps with pricing
- Is the UX bad? â† overlaps with hard to use
```

With MECE:
```
Research Question: "Why are users churning?"
Sub-questions (GOOD):
1. External factors (market, competition)
2. Acquisition issues (wrong users)
3. Activation issues (never got value)
4. Product issues (got value, then problems)
5. Support issues (couldn't get help)
```

### MECE Decomposition Patterns

#### Pattern 1: Lifecycle/Process
Break by stages of a process:

```
Before â†’ During â†’ After
Input â†’ Process â†’ Output
Acquire â†’ Activate â†’ Retain â†’ Revenue â†’ Refer
```

**Example:** "Why is conversion low?"
```
â”œâ”€â”€ Awareness stage issues
â”œâ”€â”€ Consideration stage issues
â”œâ”€â”€ Decision stage issues
â””â”€â”€ Purchase stage issues
```

#### Pattern 2: Internal vs External
Break by source/origin:

```
Internal factors (within control)
External factors (outside control)
```

**Example:** "Why did the project fail?"
```
â”œâ”€â”€ Internal factors
â”‚   â”œâ”€â”€ Team issues
â”‚   â”œâ”€â”€ Technical issues
â”‚   â””â”€â”€ Process issues
â””â”€â”€ External factors
    â”œâ”€â”€ Market changes
    â”œâ”€â”€ Competitor actions
    â””â”€â”€ Regulatory changes
```

#### Pattern 3: Quantitative vs Qualitative
Break by measurement type:

```
Quantitative (measurable)
Qualitative (subjective)
```

**Example:** "How good is this solution?"
```
â”œâ”€â”€ Quantitative metrics
â”‚   â”œâ”€â”€ Performance
â”‚   â”œâ”€â”€ Cost
â”‚   â””â”€â”€ Scalability
â””â”€â”€ Qualitative factors
    â”œâ”€â”€ Developer experience
    â”œâ”€â”€ Community support
    â””â”€â”€ Future trajectory
```

#### Pattern 4: Problem/Solution
Break by diagnosis vs prescription:

```
What is the problem?
What are the solutions?
What is the recommendation?
```

#### Pattern 5: Stakeholder-Based
Break by who is affected:

```
Users
Business
Engineering
Operations
```

#### Pattern 6: Time-Based
Break by temporal dimension:

```
Past (what happened)
Present (current state)
Future (trajectory)
```

### MECE Examples by Research Type

#### Comparative Analysis MECE
```
Compare X vs Y
â”œâ”€â”€ What they do (capabilities)
â”œâ”€â”€ How they differ (differentiators)
â”œâ”€â”€ Where they fall short (limitations)
â”œâ”€â”€ When to use each (context)
â””â”€â”€ What others say (validation)
```

#### Feasibility Study MECE
```
Is X feasible?
â”œâ”€â”€ Technical viability
â”œâ”€â”€ Resource requirements
â”œâ”€â”€ Risk factors
â”œâ”€â”€ Alternatives
â””â”€â”€ Go/no-go factors
```

#### Cost-Benefit MECE
```
Is X worth it?
â”œâ”€â”€ Direct costs
â”œâ”€â”€ Indirect costs
â”œâ”€â”€ Direct benefits
â”œâ”€â”€ Indirect benefits
â””â”€â”€ Net assessment
```

---

## Hypothesis-Driven Investigation

### When to Use

| Use Hypothesis-Driven | Use Exploratory |
|-----------------------|-----------------|
| User has a belief to test | Unknown territory |
| Specific claim to validate | Landscape mapping |
| Decision-focused | Learning-focused |
| Time-constrained | Thorough understanding needed |

### Hypothesis Format

```
Hypothesis: [Specific, falsifiable statement]
Test by examining: [What evidence would support/refute]
Decision criteria: [How to interpret results]
```

**Good Hypothesis Examples:**
- "Switching to Postgres RDS will reduce our operational overhead significantly"
- "React Native is mature enough for our enterprise mobile app requirements"
- "The performance issues are caused by N+1 queries, not database sizing"

**Bad Hypothesis Examples:**
- "React is good" (not specific)
- "We should use microservices" (not falsifiable)
- "The system is slow" (just an observation)

### Hypothesis Testing Framework

```
1. STATE HYPOTHESIS
   What do you believe? Why?

2. DEFINE EVIDENCE CRITERIA
   What would support it?
   What would refute it?
   What would be inconclusive?

3. GATHER EVIDENCE
   Look for both supporting AND contradicting evidence
   (Actively seek disconfirmation)

4. ASSESS EVIDENCE
   Weight by source quality
   Note confidence levels

5. VERDICT
   â–¡ Supported (proceed with confidence)
   â–¡ Partially supported (proceed with caveats)
   â–¡ Refuted (abandon or revise)
   â–¡ Inconclusive (need more evidence)

6. REFINE (if needed)
   Update hypothesis based on evidence
```

### Avoiding Confirmation Bias

Common traps:
- Only searching for supporting evidence
- Dismissing contradicting evidence
- Over-weighting sources that agree

Countermeasures:
- Explicitly search for contradicting evidence
- Give equal weight to all quality sources
- Steel-man the opposing view
- Ask: "What would change my mind?"

---

## Research Planning Template

Use this template to plan research before execution:

```markdown
# Research Plan

## 1. Question Definition

**Main Question:** [Clear, specific question]

**Success Criteria:** [What does a good answer look like?]

**Scope Boundaries:**
- In scope: [What to cover]
- Out of scope: [What to exclude]
- Depth: [Surface / Standard / Deep]

## 2. Mode Selection

**Mode:** â–¡ Exploratory  â–¡ Hypothesis-Driven

**If Hypothesis-Driven:**
- Hypothesis: [Statement]
- Support evidence: [What to look for]
- Refute evidence: [What would disprove]

## 3. MECE Decomposition

**Sub-Questions:**
1. [Sub-Q1] â†’ Sources: [types]
2. [Sub-Q2] â†’ Sources: [types]
3. [Sub-Q3] â†’ Sources: [types]
4. [Sub-Q4] â†’ Sources: [types]
5. [Sub-Q5] â†’ Sources: [types]

**MECE Check:**
- [ ] No overlaps between sub-questions
- [ ] All aspects of main question covered

## 4. Source Strategy

| Sub-Question | Source Types | Quality Threshold | Min Sources |
|--------------|--------------|-------------------|-------------|
| SQ1 | [types] | ğŸŸ¢/ğŸŸ¡/ğŸ”´ | [N] |
| SQ2 | [types] | ğŸŸ¢/ğŸŸ¡/ğŸ”´ | [N] |
| ... | ... | ... | ... |

## 5. Quality Checkpoints

After initial pass:
- [ ] All sub-questions addressed
- [ ] Minimum sources per claim
- [ ] Contradictions noted
- [ ] Gaps identified

## 6. Deliverable Structure

- [ ] TL;DR
- [ ] Executive Summary  
- [ ] [Section 1]
- [ ] [Section 2]
- [ ] So What section
- [ ] Methodology
- [ ] Sources
```

---

## Issue Trees

Issue trees are visual MECE decompositions used in consulting:

### Problem-Solving Issue Tree

```
Problem: Revenue is declining
â”‚
â”œâ”€â”€ Volume declining?
â”‚   â”œâ”€â”€ Market shrinking?
â”‚   â”œâ”€â”€ Losing share?
â”‚   â””â”€â”€ Mix shifting?
â”‚
â””â”€â”€ Price declining?
    â”œâ”€â”€ Competitive pressure?
    â”œâ”€â”€ Mix shifting?
    â””â”€â”€ Discounting?
```

### Solution Issue Tree

```
How to grow revenue?
â”‚
â”œâ”€â”€ Increase volume
â”‚   â”œâ”€â”€ Acquire new customers
â”‚   â”œâ”€â”€ Increase usage per customer
â”‚   â””â”€â”€ Reduce churn
â”‚
â””â”€â”€ Increase price
    â”œâ”€â”€ Raise prices
    â”œâ”€â”€ Shift mix to premium
    â””â”€â”€ Reduce discounting
```

### Hypothesis Tree

```
Hypothesis: We should migrate to cloud
â”‚
â”œâ”€â”€ Cost hypothesis
â”‚   â”œâ”€â”€ Reduces infrastructure cost
â”‚   â”œâ”€â”€ Reduces ops cost
â”‚   â””â”€â”€ Capex â†’ Opex benefits
â”‚
â”œâ”€â”€ Capability hypothesis
â”‚   â”œâ”€â”€ Enables scalability
â”‚   â”œâ”€â”€ Enables new features
â”‚   â””â”€â”€ Improves reliability
â”‚
â””â”€â”€ Strategic hypothesis
    â”œâ”€â”€ Industry moving this way
    â”œâ”€â”€ Talent expects it
    â””â”€â”€ Competitive necessity
```

---

## The "Five Whys" Technique

Drill down to root cause by asking "Why?" repeatedly:

```
Problem: API latency is high

Why? â†’ Database queries are slow
Why? â†’ Too many queries per request
Why? â†’ N+1 query pattern in the code
Why? â†’ ORM eager loading not configured
Why? â†’ Team wasn't aware of the pattern

Root Cause: Knowledge gap in team
Solution: Training + code review checklist
```

---

## Quick Reference: Planning Checklist

Before starting any research:

```
â–¡ Question is specific and answerable
â–¡ Scope is defined (in/out)
â–¡ Mode selected (exploratory/hypothesis)
â–¡ Sub-questions are MECE
â–¡ Sources identified per sub-question
â–¡ Quality thresholds set
â–¡ Deliverable structure planned
â–¡ Success criteria defined
```
